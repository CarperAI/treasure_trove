[
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "treasure_trove",
    "section": "",
    "text": "from datasets import load_dataset\nfrom squeakily.helpers import LLMLabeler\nfrom transformers import pipeline, TrainingArguments\nfrom treasure_trove.core import filter_dataset, label_dataset, train_labeler\n\ninstruction = \"\"\"Please label the following code as either educational or non-educational.\nEducational code is code that is well written, follows best practices, has documentation such that it might be found in a textbook.\nNon-educational code is code that is poorly written, lacks documentation, contain bugs, or is not idiomatic.\nLabels:\n\"\"\"\nlabels = [\"educational\", \"non-educational\"]\napi_key = \"&lt;api_key&gt;\"\nlabeler = LLMLabeler(instruction, labels, model_name=\"gpt-4\", api_key=api_key)\n\nds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\")[\"train\"]\n\n# Get the training arguments\nbatch_size=4,\ntraining_args = TrainingArguments(\n    output_dir=\"./code_edu\",\n    num_train_epochs=1,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    seed=42,\n    push_to_hub=True,\n)\n\n\nsubset = label_dataset(ds, \"content\", labeler, labels, sample=0.001)\nbase_model_name = \"bigcode/starencoder\"\nmodel, tokenizer = train_labeler(\n    subset,\n    \"content\",\n    base_model_name,\n    n_labels=len(labels),\n    training_args=training_args,\n    num_workers=4,\n    max_length=512,\n    push_to_hub=True,\n)\npipe = pipeline(\n    \"text-classification\", model=model, tokenizer=tokenizer, device=model.device\n)\nfiltered_ds = filter_dataset(ds, \"content\", model, labels.index(\"educational\"))\nfiltered_ds.push_to_hub(\"ncoop57/code_edu\")"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nclassify\n\n classify (x, labels, llm_labeler, max_failures=5, default_label=0)\n\n\nsource\n\n\nlabel_dataset\n\n label_dataset (dataset, text_column, labeler_model, labels, sample=0.1,\n                num_workers=4, max_chars=4096)\n\nFilters a dataset using a labeler model.\nArgs: dataset (datasets.Dataset): Dataset to filter text_column (str): Name of the column containing the text to classify labeler_model (Any): Model to use for labeling labels (List[str]): List of labels sample (float): The fraction of the dataset to label and use for filtering batch_size (int): Batch size for labeling num_workers (int): Number of workers for labeling max_chars (int): Maximum number of characters to truncate the text to before labeling (reduces rate limiting errors)\n\nfrom functools import partial\nfrom datasets import load_dataset\n\n\ndef mock_labeler(x, labels):\n    return [np.random.choice(labels, p=[0.25, 0.75])]\n\n\nlabels = [\"positive\", \"negative\"]\nlabeler = partial(mock_labeler, labels=labels)\nds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\")[\"train\"]\n\nsubset = label_dataset(ds, \"content\", labeler, labels, sample=0.1)\n\nassert \"label\" in subset.column_names\n\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n│ in &lt;module&gt;:11                                                               │\n│                                                                              │\n│ /opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/datasets/ │\n│ load.py:1718 in load_dataset                                                 │\n│                                                                              │\n│   1715 │   ignore_verifications = ignore_verifications or save_infos         │\n│   1716 │                                                                     │\n│   1717 │   # Create a dataset builder                                        │\n│ ❱ 1718 │   builder_instance = load_dataset_builder(                          │\n│   1719 │   │   path=path,                                                    │\n│   1720 │   │   name=name,                                                    │\n│   1721 │   │   data_dir=data_dir,                                            │\n│                                                                              │\n│ /opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/datasets/ │\n│ load.py:1488 in load_dataset_builder                                         │\n│                                                                              │\n│   1485 │   if use_auth_token is not None:                                    │\n│   1486 │   │   download_config = download_config.copy() if download_config e │\n│   1487 │   │   download_config.use_auth_token = use_auth_token               │\n│ ❱ 1488 │   dataset_module = dataset_module_factory(                          │\n│   1489 │   │   path,                                                         │\n│   1490 │   │   revision=revision,                                            │\n│   1491 │   │   download_config=download_config,                              │\n│                                                                              │\n│ /opt/hostedtoolcache/Python/3.9.17/x64/lib/python3.9/site-packages/datasets/ │\n│ load.py:1212 in dataset_module_factory                                       │\n│                                                                              │\n│   1209 │   │   │   │   if isinstance(e1, EmptyDatasetError):                 │\n│   1210 │   │   │   │   │   raise e1 from None                                │\n│   1211 │   │   │   │   if isinstance(e1, FileNotFoundError):                 │\n│ ❱ 1212 │   │   │   │   │   raise FileNotFoundError(                          │\n│   1213 │   │   │   │   │   │   f\"Couldn't find a dataset script at {relative │\n│   1214 │   │   │   │   │   │   f\"Couldn't find '{path}' on the Hugging Face  │\n│   1215 │   │   │   │   │   ) from None                                       │\n╰──────────────────────────────────────────────────────────────────────────────╯\nFileNotFoundError: Couldn't find a dataset script at \n/home/runner/work/treasure_trove/treasure_trove/bigcode/the-stack-smol/the-stack\n-smol.py or any data file in the same directory. Couldn't find \n'bigcode/the-stack-smol' on the Hugging Face Hub either: FileNotFoundError: \nDataset 'bigcode/the-stack-smol' doesn't exist on the Hub. If the repo is \nprivate, make sure you are authenticated with `use_auth_token=True` after \nlogging in with `huggingface-cli login`.\n\n\n\nsource\n\n\ntrain_labeler\n\n train_labeler (dataset, text_column, base_model_name, n_labels,\n                training_args, num_workers=4, max_length=512,\n                push_to_hub=True)\n\nTrains a labeler model on a labeled dataset.\nArgs: dataset (datasets.Dataset): Dataset to train on text_column (str): Name of the text column base_model_name (str): Name of the base model to use n_labels (int): Number of labels epochs (int): Number of epochs to train batch_size (int): Batch size for training num_workers (int): Number of workers for training max_length (int): Maximum length of the input\n\nfrom transformers import pipeline\n\nbase_model_name = \"prajjwal1/bert-small\"\nmodel, tokenizer = train_labeler(\n    subset,\n    \"content\",\n    base_model_name,\n    n_labels=len(labels),\n    epochs=1,\n    batch_size=4,\n    num_workers=4,\n)\nassert type(model) == AutoModelForSequenceClassification\n\n╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n│ in &lt;module&gt;:5                                                                │\n╰──────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'subset' is not defined\n\n\n\nsource\n\n\nfilter_dataset\n\n filter_dataset (dataset, text_column, labeler_model, labels_to_keep,\n                 batch_size=32, num_workers=4)\n\nFilters a dataset using a labeler model.\nArgs: dataset (datasets.Dataset): Dataset to filter text_column (str): Name of the text column labeler_model (transformers.pipelines.TextClassificationPipeline): Model to use for labeling labels_to_keep (list): List of labels to keep batch_size (int): Batch size for labeling num_workers (int): Number of workers for labeling\n\npipe = pipeline(\n    \"text-classification\", model=model, tokenizer=tokenizer, device=model.device\n)\nfiltered_ds = filter_dataset(ds, \"content\", pipe, [0])\n\nassert len(filtered_ds) &lt; len(ds)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "treasure_trove",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "treasure_trove",
    "section": "Install",
    "text": "Install\npip install treasure_trove"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "treasure_trove",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1 + 1\n\n2"
  }
]