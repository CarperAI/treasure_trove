{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import evaluate\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def classify(x, labels, llm_labeler, max_failures=5, default_label=0):\n",
    "    failures = 0\n",
    "    while failures < max_failures:\n",
    "        try:\n",
    "            return labels.index(llm_labeler(x)[0])\n",
    "        except Exception as e:\n",
    "            failures += 1\n",
    "            print(e)\n",
    "            time.sleep(1)\n",
    "            pass\n",
    "    if failures == max_failures:\n",
    "        return default_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def label_dataset(\n",
    "    dataset, text_column, labeler_model, labels, sample=0.1, num_workers=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters a dataset using a labeler model.\n",
    "\n",
    "    Args:\n",
    "        dataset (datasets.Dataset): Dataset to filter\n",
    "        text_column (str): Name of the column containing the text to classify\n",
    "        labeler_model (Any): Model to use for labeling\n",
    "        labels (List[str]): List of labels\n",
    "        sample (float): The fraction of the dataset to label and use for filtering\n",
    "        batch_size (int): Batch size for labeling\n",
    "        num_workers (int): Number of workers for labeling\n",
    "    \"\"\"\n",
    "\n",
    "    # Get a subset of the dataset\n",
    "    subset = dataset.shuffle(seed=115).select(range(int(len(dataset) * sample)))\n",
    "\n",
    "    # Label the subset\n",
    "    subset = subset.map(\n",
    "        lambda x: {\"label\": classify(x[text_column], labels, labeler_model)},\n",
    "        batched=False,\n",
    "        num_proc=num_workers,\n",
    "    )\n",
    "\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bigcode--the-stack-smol-8f8055c3a4e4b4e3\n",
      "Found cached dataset json (/home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-8f8055c3a4e4b4e3/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb95116fc20477bb047848972658d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/nathan/.cache/huggingface/datasets/bigcode___json/bigcode--the-stack-smol-8f8055c3a4e4b4e3/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-feaf44b92e145e5a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9033fc6799034c8abffcb46335958b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def mock_labeler(x, labels):\n",
    "    return [np.random.choice(labels, p=[0.25, 0.75])]\n",
    "\n",
    "\n",
    "labels = [\"positive\", \"negative\"]\n",
    "labeler = partial(mock_labeler, labels=labels)\n",
    "ds = load_dataset(\"bigcode/the-stack-smol\", data_dir=\"data/python\")[\"train\"]\n",
    "\n",
    "subset = label_dataset(ds, \"content\", labeler, labels, sample=0.1)\n",
    "\n",
    "assert \"label\" in subset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def train_labeler(\n",
    "    dataset,\n",
    "    text_column,\n",
    "    base_model_name,\n",
    "    n_labels,\n",
    "    training_args,\n",
    "    num_workers=4,\n",
    "    max_length=512,\n",
    "    push_to_hub=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a labeler model on a labeled dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (datasets.Dataset): Dataset to train on\n",
    "        text_column (str): Name of the text column\n",
    "        base_model_name (str): Name of the base model to use\n",
    "        n_labels (int): Number of labels\n",
    "        epochs (int): Number of epochs to train\n",
    "        batch_size (int): Batch size for training\n",
    "        num_workers (int): Number of workers for training\n",
    "        max_length (int): Maximum length of the input\n",
    "    \"\"\"\n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, max_length=max_length)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name, num_labels=n_labels, max_length=max_length\n",
    "    )\n",
    "    model.config.id2label = {i: i for i in range(n_labels)}\n",
    "\n",
    "    # Preprocess the dataset\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[text_column], padding=\"max_length\", truncation=True, max_length=max_length\n",
    "        ),\n",
    "        batched=True,\n",
    "        num_proc=num_workers,\n",
    "    )\n",
    "\n",
    "    # Split the dataset\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    # Get the data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Get the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Push the model to the hub\n",
    "    if push_to_hub:\n",
    "        trainer.push_to_hub()\n",
    "\n",
    "    # Return the model\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008f5b697a4a469d8e9e113140ff1938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/trove/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ca5beb1fbb4df9a9b871e9b929d5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7437, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 0.711, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.09}\n",
      "{'loss': 0.6896, 'learning_rate': 3e-06, 'epoch': 0.13}\n",
      "{'loss': 0.6414, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.18}\n",
      "{'loss': 0.6547, 'learning_rate': 5e-06, 'epoch': 0.22}\n",
      "{'loss': 0.5845, 'learning_rate': 6e-06, 'epoch': 0.27}\n",
      "{'loss': 0.5528, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 0.6287, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.36}\n",
      "{'loss': 0.6309, 'learning_rate': 9e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6, 'learning_rate': 1e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6651, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.49}\n",
      "{'loss': 0.5361, 'learning_rate': 1.2e-05, 'epoch': 0.53}\n",
      "{'loss': 0.674, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.58}\n",
      "{'loss': 0.6853, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6342, 'learning_rate': 1.5e-05, 'epoch': 0.67}\n",
      "{'loss': 0.6266, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.71}\n",
      "{'loss': 0.4705, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5439, 'learning_rate': 1.8e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4427, 'learning_rate': 1.9e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5829, 'learning_rate': 2e-05, 'epoch': 0.89}\n",
      "{'loss': 0.5624, 'learning_rate': 2.1e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6028, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72fabdbb3aa4dc8903d52013aa42f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5282490253448486, 'eval_accuracy': 0.8, 'eval_f1': 0.888888888888889, 'eval_runtime': 0.933, 'eval_samples_per_second': 107.178, 'eval_steps_per_second': 26.794, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19.5343, 'train_samples_per_second': 46.073, 'train_steps_per_second': 11.518, 'train_loss': 0.6103349855211047, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "base_model_name = \"prajjwal1/bert-small\"\n",
    "model, tokenizer = train_labeler(\n",
    "    subset,\n",
    "    \"content\",\n",
    "    base_model_name,\n",
    "    n_labels=len(labels),\n",
    "    epochs=1,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    ")\n",
    "assert type(model) == AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def filter_dataset(\n",
    "    dataset, text_column, labeler_model, labels_to_keep, batch_size=32, num_workers=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Filters a dataset using a labeler model.\n",
    "\n",
    "    Args:\n",
    "        dataset (datasets.Dataset): Dataset to filter\n",
    "        text_column (str): Name of the text column\n",
    "        labeler_model (transformers.pipelines.TextClassificationPipeline): Model to use for labeling\n",
    "        labels_to_keep (list): List of labels to keep\n",
    "        batch_size (int): Batch size for labeling\n",
    "        num_workers (int): Number of workers for labeling\n",
    "    \"\"\"\n",
    "\n",
    "    def label(x):\n",
    "        predicted = labeler_model(x, padding=True, truncation=True, max_length=512)\n",
    "        return {\n",
    "            \"label\": [l[\"label\"] for l in predicted],\n",
    "            \"score\": [l[\"score\"] for l in predicted],\n",
    "        }\n",
    "\n",
    "    # Label the dataset\n",
    "    dataset = dataset.map(\n",
    "        lambda x: label(x[text_column]),\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        num_proc=num_workers,\n",
    "    )\n",
    "\n",
    "    # Filter the dataset\n",
    "    dataset = dataset.filter(lambda x: x[\"label\"] in labels_to_keep)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14fd4c3288947358f8b9e01c4a50655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2897d09fb0a409793cd5bf9855e5999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-classification\", model=model, tokenizer=tokenizer, device=model.device\n",
    ")\n",
    "filtered_ds = filter_dataset(ds, \"content\", pipe, [0])\n",
    "\n",
    "assert len(filtered_ds) < len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
